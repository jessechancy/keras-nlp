{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLMRCheckpointConversion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "Tp-yzo1jZeMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras-nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICK2t0tuZlXw",
        "outputId": "376cb6d1-99db-4c41-e58b-83938f4357ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 142 kB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 45.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 5.3 kB/s \n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 4.5 kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 31.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ix-MBkwZWS_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_nlp\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Model"
      ],
      "metadata": {
        "id": "C47QCBvSZpt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class XLMRModel(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size, #250002\n",
        "        num_layers=12,\n",
        "        hidden_size=768,\n",
        "        dropout=0.1,\n",
        "        num_attention_heads=12,\n",
        "        inner_size=3072,\n",
        "        inner_activation=\"gelu\",\n",
        "        initializer_range=0.02,\n",
        "        max_sequence_length=512,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.inner_size = inner_size\n",
        "        self.inner_activation = keras.activations.get(inner_activation)\n",
        "        self.initializer_range = initializer_range\n",
        "        self.initializer = keras.initializers.TruncatedNormal(\n",
        "            stddev=initializer_range\n",
        "        )\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self._token_and_position_embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "            vocabulary_size=vocab_size,\n",
        "            sequence_length=max_sequence_length,\n",
        "            embedding_dim=hidden_size,\n",
        "            name=\"token_and_position_embeddings\"\n",
        "        )\n",
        "\n",
        "        self._embedding_norm_layer = keras.layers.LayerNormalization(\n",
        "            name=\"embeddings/layer_norm\",\n",
        "            axis=-1,\n",
        "            epsilon=1e-5,\n",
        "            dtype=tf.float32,\n",
        "        )\n",
        "\n",
        "        self._embedding_dropout = keras.layers.Dropout(\n",
        "            rate=dropout, name=\"embedding_dropout\"\n",
        "        )\n",
        "\n",
        "        self._transformer_layers = []\n",
        "        for i in range(num_layers):\n",
        "            layer = keras_nlp.layers.TransformerEncoder(\n",
        "                num_heads=num_attention_heads,\n",
        "                intermediate_dim=inner_size,\n",
        "                activation=self.inner_activation,\n",
        "                dropout=dropout,\n",
        "                kernel_initializer=self.initializer,\n",
        "                name=\"transformer/layer_%d\" % i,\n",
        "            )\n",
        "            self._transformer_layers.append(layer)\n",
        "\n",
        "        self.inputs = dict(\n",
        "            input_ids=keras.Input(shape=(None,), dtype=tf.int32),\n",
        "            input_mask=keras.Input(shape=(None,), dtype=tf.int32),\n",
        "            segment_ids=keras.Input(shape=(None,), dtype=tf.int32),\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if isinstance(inputs, dict):\n",
        "            input_ids = inputs.get(\"input_ids\")\n",
        "            input_mask = inputs.get(\"input_mask\")\n",
        "        else:\n",
        "            raise ValueError(f\"Inputs should be a dict. Received: {inputs}.\")\n",
        "\n",
        "        embeddings = self._token_and_position_embedding_layer(input_ids)\n",
        "        embeddings = self._embedding_norm_layer(embeddings)\n",
        "        embeddings = self._embedding_dropout(embeddings)\n",
        "\n",
        "        x = embeddings\n",
        "        for layer in self._transformer_layers:\n",
        "            x = layer(x, padding_mask=input_mask)\n",
        "        sequence_output = x\n",
        "        return sequence_output\n",
        "\n",
        "    def get_embedding_table(self):\n",
        "        return self._token_and_position_embedding_layer.token_embedding.embeddings\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"hidden_size\": self.hidden_size,\n",
        "                \"num_layers\": self.num_layers,\n",
        "                \"num_attention_heads\": self.num_attention_heads,\n",
        "                \"max_sequence_length\": self.max_sequence_length,\n",
        "                \"inner_size\": self.inner_size,\n",
        "                \"inner_activation\": keras.activations.serialize(\n",
        "                    self.inner_activation\n",
        "                ),\n",
        "                \"dropout\": self.dropout,\n",
        "                \"initializer_range\": self.initializer_range,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "kfEWj-jFZhip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XLMRModel(vocab_size=250002)\n",
        "model(dict(\n",
        "  input_ids=keras.Input(shape=(None,), dtype=tf.int32),\n",
        "  input_mask=keras.Input(shape=(None,), dtype=tf.int32),\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw3ZfmPwZvQH",
        "outputId": "075382ee-7972-40d6-c689-29176ac13aea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, None, 768) dtype=float32 (created by layer 'xlmr_model_1')>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWczL5sdZz2q",
        "outputId": "33a7a2d4-836b-4bd2-8ba9-14f249510801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"xlmr_model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " token_and_position_embeddin  multiple                 192394752 \n",
            " gs (TokenAndPositionEmbeddi                                     \n",
            " ng)                                                             \n",
            "                                                                 \n",
            " embeddings/layer_norm (Laye  multiple                 1536      \n",
            " rNormalization)                                                 \n",
            "                                                                 \n",
            " embedding_dropout (Dropout)  multiple                 0         \n",
            "                                                                 \n",
            " transformer/layer_0 (Transf  multiple                 7087872   \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer/layer_1 (Transf  multiple                 7087872   \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer/layer_2 (Transf  multiple                 7087872   \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer/layer_3 (Transf  multiple                 7087872   \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer/layer_4 (Transf  multiple                 7087872   \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer/layer_5 (Transf  multiple                 7087872   \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer/layer_6 (Transf  multiple                 7087872   \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer/layer_7 (Transf  multiple                 7087872   \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer/layer_8 (Transf  multiple                 7087872   \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer/layer_9 (Transf  multiple                 7087872   \n",
            " ormerEncoder)                                                   \n",
            "                                                                 \n",
            " transformer/layer_10 (Trans  multiple                 7087872   \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " transformer/layer_11 (Trans  multiple                 7087872   \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 277,450,752\n",
            "Trainable params: 277,450,752\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load PyTorch Checkpoints"
      ],
      "metadata": {
        "id": "oTNvzbLFZ3Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"drive/MyDrive/xlmr.base/model.pt\", map_location=torch.device('cpu'))\n",
        "ckp = checkpoint['model'] # ckp used later"
      ],
      "metadata": {
        "id": "PMy8OsyQZ2jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckp.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6HkNrBMeqN1",
        "outputId": "799ad60b-b58f-401b-f6b1-d922a9aa926a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['decoder.sentence_encoder.embed_tokens.weight', 'decoder.sentence_encoder.embed_positions.weight', 'decoder.sentence_encoder.layers.0.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.0.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.0.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.0.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.0.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.0.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.0.fc1.weight', 'decoder.sentence_encoder.layers.0.fc1.bias', 'decoder.sentence_encoder.layers.0.fc2.weight', 'decoder.sentence_encoder.layers.0.fc2.bias', 'decoder.sentence_encoder.layers.0.final_layer_norm.weight', 'decoder.sentence_encoder.layers.0.final_layer_norm.bias', 'decoder.sentence_encoder.layers.1.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.1.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.1.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.1.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.1.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.1.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.1.fc1.weight', 'decoder.sentence_encoder.layers.1.fc1.bias', 'decoder.sentence_encoder.layers.1.fc2.weight', 'decoder.sentence_encoder.layers.1.fc2.bias', 'decoder.sentence_encoder.layers.1.final_layer_norm.weight', 'decoder.sentence_encoder.layers.1.final_layer_norm.bias', 'decoder.sentence_encoder.layers.2.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.2.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.2.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.2.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.2.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.2.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.2.fc1.weight', 'decoder.sentence_encoder.layers.2.fc1.bias', 'decoder.sentence_encoder.layers.2.fc2.weight', 'decoder.sentence_encoder.layers.2.fc2.bias', 'decoder.sentence_encoder.layers.2.final_layer_norm.weight', 'decoder.sentence_encoder.layers.2.final_layer_norm.bias', 'decoder.sentence_encoder.layers.3.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.3.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.3.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.3.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.3.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.3.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.3.fc1.weight', 'decoder.sentence_encoder.layers.3.fc1.bias', 'decoder.sentence_encoder.layers.3.fc2.weight', 'decoder.sentence_encoder.layers.3.fc2.bias', 'decoder.sentence_encoder.layers.3.final_layer_norm.weight', 'decoder.sentence_encoder.layers.3.final_layer_norm.bias', 'decoder.sentence_encoder.layers.4.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.4.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.4.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.4.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.4.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.4.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.4.fc1.weight', 'decoder.sentence_encoder.layers.4.fc1.bias', 'decoder.sentence_encoder.layers.4.fc2.weight', 'decoder.sentence_encoder.layers.4.fc2.bias', 'decoder.sentence_encoder.layers.4.final_layer_norm.weight', 'decoder.sentence_encoder.layers.4.final_layer_norm.bias', 'decoder.sentence_encoder.layers.5.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.5.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.5.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.5.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.5.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.5.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.5.fc1.weight', 'decoder.sentence_encoder.layers.5.fc1.bias', 'decoder.sentence_encoder.layers.5.fc2.weight', 'decoder.sentence_encoder.layers.5.fc2.bias', 'decoder.sentence_encoder.layers.5.final_layer_norm.weight', 'decoder.sentence_encoder.layers.5.final_layer_norm.bias', 'decoder.sentence_encoder.layers.6.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.6.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.6.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.6.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.6.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.6.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.6.fc1.weight', 'decoder.sentence_encoder.layers.6.fc1.bias', 'decoder.sentence_encoder.layers.6.fc2.weight', 'decoder.sentence_encoder.layers.6.fc2.bias', 'decoder.sentence_encoder.layers.6.final_layer_norm.weight', 'decoder.sentence_encoder.layers.6.final_layer_norm.bias', 'decoder.sentence_encoder.layers.7.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.7.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.7.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.7.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.7.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.7.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.7.fc1.weight', 'decoder.sentence_encoder.layers.7.fc1.bias', 'decoder.sentence_encoder.layers.7.fc2.weight', 'decoder.sentence_encoder.layers.7.fc2.bias', 'decoder.sentence_encoder.layers.7.final_layer_norm.weight', 'decoder.sentence_encoder.layers.7.final_layer_norm.bias', 'decoder.sentence_encoder.layers.8.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.8.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.8.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.8.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.8.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.8.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.8.fc1.weight', 'decoder.sentence_encoder.layers.8.fc1.bias', 'decoder.sentence_encoder.layers.8.fc2.weight', 'decoder.sentence_encoder.layers.8.fc2.bias', 'decoder.sentence_encoder.layers.8.final_layer_norm.weight', 'decoder.sentence_encoder.layers.8.final_layer_norm.bias', 'decoder.sentence_encoder.layers.9.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.9.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.9.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.9.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.9.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.9.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.9.fc1.weight', 'decoder.sentence_encoder.layers.9.fc1.bias', 'decoder.sentence_encoder.layers.9.fc2.weight', 'decoder.sentence_encoder.layers.9.fc2.bias', 'decoder.sentence_encoder.layers.9.final_layer_norm.weight', 'decoder.sentence_encoder.layers.9.final_layer_norm.bias', 'decoder.sentence_encoder.layers.10.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.10.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.10.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.10.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.10.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.10.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.10.fc1.weight', 'decoder.sentence_encoder.layers.10.fc1.bias', 'decoder.sentence_encoder.layers.10.fc2.weight', 'decoder.sentence_encoder.layers.10.fc2.bias', 'decoder.sentence_encoder.layers.10.final_layer_norm.weight', 'decoder.sentence_encoder.layers.10.final_layer_norm.bias', 'decoder.sentence_encoder.layers.11.self_attn.in_proj_weight', 'decoder.sentence_encoder.layers.11.self_attn.in_proj_bias', 'decoder.sentence_encoder.layers.11.self_attn.out_proj.weight', 'decoder.sentence_encoder.layers.11.self_attn.out_proj.bias', 'decoder.sentence_encoder.layers.11.self_attn_layer_norm.weight', 'decoder.sentence_encoder.layers.11.self_attn_layer_norm.bias', 'decoder.sentence_encoder.layers.11.fc1.weight', 'decoder.sentence_encoder.layers.11.fc1.bias', 'decoder.sentence_encoder.layers.11.fc2.weight', 'decoder.sentence_encoder.layers.11.fc2.bias', 'decoder.sentence_encoder.layers.11.final_layer_norm.weight', 'decoder.sentence_encoder.layers.11.final_layer_norm.bias', 'decoder.sentence_encoder.emb_layer_norm.weight', 'decoder.sentence_encoder.emb_layer_norm.bias', 'decoder.lm_head.weight', 'decoder.lm_head.bias', 'decoder.lm_head.dense.weight', 'decoder.lm_head.dense.bias', 'decoder.lm_head.layer_norm.weight', 'decoder.lm_head.layer_norm.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get TF RoBERTa layers"
      ],
      "metadata": {
        "id": "WFzAmrEqaweZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer2name = dict()\n",
        "layer2shape = dict()\n",
        "for layer in model.layers:\n",
        "  layer2name[layer] = list(map(lambda x: x.name, layer.weights))\n",
        "  layer2shape[layer] = list(map(lambda x: x.shape, layer.weights))"
      ],
      "metadata": {
        "id": "Y7rShc5Harb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpOKfTWLa6vB",
        "outputId": "b855fdf8-6ddf-4013-98f2-9f41691ee2a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras_nlp.layers.token_and_position_embedding.TokenAndPositionEmbedding at 0x7efe97eb1990>,\n",
              " <keras.layers.normalization.layer_normalization.LayerNormalization at 0x7efe964a4a50>,\n",
              " <keras.layers.core.dropout.Dropout at 0x7efe964a4890>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe964a4090>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe97e50c50>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe964a7290>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe964a7d10>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe964a7550>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe964a7bd0>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe96511dd0>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe96a04bd0>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe975e80d0>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe97624f90>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe96511b90>,\n",
              " <keras_nlp.layers.transformer_encoder.TransformerEncoder at 0x7efe96a54a10>]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert PyTorch Tensor to TF Tensor"
      ],
      "metadata": {
        "id": "uTnASI4VbBbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Layers"
      ],
      "metadata": {
        "id": "5PN99M25bCfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenandpositionembedding_layer = model.layers[0] #TokenAndPositionEmbedding"
      ],
      "metadata": {
        "id": "usTF5RJIbFQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_tensor = ckp['decoder.sentence_encoder.embed_tokens.weight'].numpy()\n",
        "position_tensor = ckp['decoder.sentence_encoder.embed_positions.weight'].numpy()[2:, :]\n",
        "embedding_tensor.shape, position_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVsqWbbMbHF5",
        "outputId": "d0907e14-51ea-4305-af62-f90f0f2ab631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((250002, 768), (512, 768))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenandpositionembedding_layer.set_weights([embedding_tensor, position_tensor])"
      ],
      "metadata": {
        "id": "N3x1MWr4bJr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layernorm = model.layers[1]"
      ],
      "metadata": {
        "id": "qA2flGRJbLtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma_tensor = ckp['decoder.sentence_encoder.emb_layer_norm.weight']\n",
        "beta_tensor = ckp['decoder.sentence_encoder.emb_layer_norm.bias']"
      ],
      "metadata": {
        "id": "ooL8_PwmbNUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layernorm.set_weights([gamma_tensor, beta_tensor])"
      ],
      "metadata": {
        "id": "Hc_kxxOWbPE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Layers"
      ],
      "metadata": {
        "id": "1iFeCqQrbSRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(12):\n",
        "  transformer_layer = model.layers[i+3]\n",
        "\n",
        "  size = 768\n",
        "  # query\n",
        "  query_weights = ckp[f'decoder.sentence_encoder.layers.{i}.self_attn.in_proj_weight'].numpy()\n",
        "  query_weights = (query_weights.T)[:, :size].reshape(768, 12, 64)\n",
        "  query_bias = ckp[f'decoder.sentence_encoder.layers.{i}.self_attn.in_proj_bias'].numpy()\n",
        "  query_bias = query_bias[:size].reshape(12, 64)\n",
        "  # key\n",
        "  key_weights = ckp[f'decoder.sentence_encoder.layers.{i}.self_attn.in_proj_weight'].numpy()\n",
        "  key_weights = (key_weights.T)[:, size:size*2].reshape(768, 12, 64)\n",
        "  key_bias = ckp[f'decoder.sentence_encoder.layers.{i}.self_attn.in_proj_bias'].numpy()\n",
        "  key_bias = key_bias[size:size*2].reshape(12, 64)\n",
        "  # value\n",
        "  value_weights = ckp[f'decoder.sentence_encoder.layers.{i}.self_attn.in_proj_weight'].numpy()\n",
        "  value_weights = (value_weights.T)[:, size*2:size*3].reshape(768, 12, 64)\n",
        "  value_bias = ckp[f'decoder.sentence_encoder.layers.{i}.self_attn.in_proj_bias'].numpy()\n",
        "  value_bias = value_bias[size*2:size*3].reshape(12, 64)\n",
        "  # attention output\n",
        "  attention_weight = ckp[f'decoder.sentence_encoder.layers.{i}.self_attn.out_proj.weight'].numpy()\n",
        "  attention_weight = attention_weight.T.reshape(12, 64, 768)\n",
        "  attention_bias = ckp[f'decoder.sentence_encoder.layers.{i}.self_attn.out_proj.bias'].numpy()\n",
        "  # layer norms\n",
        "  layernorm_1_gamma = ckp[f'decoder.sentence_encoder.layers.{i}.self_attn_layer_norm.weight'].numpy()\n",
        "  layernorm_1_beta = ckp[f'decoder.sentence_encoder.layers.{i}.self_attn_layer_norm.bias'].numpy()\n",
        "  layernorm_2_gamma = ckp[f'decoder.sentence_encoder.layers.{i}.final_layer_norm.weight'].numpy()\n",
        "  layernorm_2_beta = ckp[f'decoder.sentence_encoder.layers.{i}.final_layer_norm.bias'].numpy()\n",
        "  # dense\n",
        "  dense_1_weight = ckp[f'decoder.sentence_encoder.layers.{i}.fc1.weight'].numpy().T\n",
        "  dense_1_bias = ckp[f'decoder.sentence_encoder.layers.{i}.fc1.bias'].numpy()\n",
        "  dense_2_weight = ckp[f'decoder.sentence_encoder.layers.{i}.fc2.weight'].numpy().T\n",
        "  dense_2_bias = ckp[f'decoder.sentence_encoder.layers.{i}.fc2.bias'].numpy()\n",
        "\n",
        "  weights = [\n",
        "      query_weights, query_bias, key_weights, key_bias, value_weights, value_bias,\n",
        "      attention_weight, attention_bias, \n",
        "      layernorm_1_gamma, layernorm_1_beta, layernorm_2_gamma, layernorm_2_beta,\n",
        "      dense_1_weight, dense_1_bias, dense_2_weight, dense_2_bias\n",
        "  ]\n",
        "\n",
        "  transformer_layer.set_weights(weights)"
      ],
      "metadata": {
        "id": "LFse5Ad9bQ1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('drive/MyDrive/tf_xlmr_ckp')"
      ],
      "metadata": {
        "id": "dO77fiV3bVQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}